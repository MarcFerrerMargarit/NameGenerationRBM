{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Name Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from GridEncoder import GridEncoder\n",
    "import Utils\n",
    "from ShortTextCodec import ShortTextCodec, BinomialShortTextCodec\n",
    "from RBM import BernoulliRBM\n",
    "import Sampling\n",
    "import sample\n",
    "import sys\n",
    "import colorama\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIASED_PRIOR = 0\n",
    "\n",
    "class CharBernoulli(BernoulliRBM):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        codec is the ShortTextCodec used to create the vectors being fit. The\n",
    "        most important function of the codec is as a proxy to the shape of the\n",
    "        softmax units in the visible layer (if you're using the CharBernoulliRBMSoftmax\n",
    "        subclass). It's also used to decode and print\n",
    "        fantasy particles at the end of each epoch.\n",
    "        \"\"\"\n",
    "        # Attaching this to the object is really helpful later on when models\n",
    "        # are loaded from pickle in visualize.py and sample.py\n",
    "        self.codec = kwargs.pop(\"codec\")\n",
    "        self.softmax_shape = codec.shape()\n",
    "        # Old-style class :(\n",
    "        BernoulliRBM.__init__(self, **kwargs)\n",
    "\n",
    "    def wellness_check(self, epoch, duration, train, validation):\n",
    "        BernoulliRBM.wellness_check(self, epoch, duration, train, validation)\n",
    "        fantasy_samples = '|'.join([self.codec.decode(vec) for vec in\n",
    "                                    self._sample_visibles(self.h_samples_[:3], temperature=0.1)])\n",
    "        print (\"Fantasy samples: {}\".format(fantasy_samples))\n",
    "\n",
    "    def corrupt(self, v):\n",
    "        n_softmax, n_opts = self.softmax_shape\n",
    "        # Select a random index in to the indices of the non-zero values of each input\n",
    "        # TODO: In the char-RBM case, if I wanted to really challenge the model, I would avoid selecting any\n",
    "        # trailing spaces here. Cause any dumb model can figure out that it should assign high energy to\n",
    "        # any instance of /  [^ ]/\n",
    "        meta_indices_to_corrupt = self.rng_.randint(0, n_softmax, v.shape[0]) + np.arange(0, n_softmax * v.shape[0], n_softmax)\n",
    "\n",
    "        # Offset these indices by a random amount (but not 0 - we want to actually change them)\n",
    "        offsets = self.rng_.randint(1, n_opts, v.shape[0])\n",
    "        # Also, do some math to make sure we don't \"spill over\" into a different softmax.\n",
    "        # E.g. if n_opts=5, and we're corrupting index 3, we should choose offsets from {-3, -2, -1, +1}\n",
    "        # 1-d array that matches with meta_i_t_c but which contains the indices themselves\n",
    "        indices_to_corrupt = v.indices[meta_indices_to_corrupt]\n",
    "        # Sweet lucifer\n",
    "        offsets = offsets - (n_opts * (((indices_to_corrupt % n_opts) + offsets.ravel()) >= n_opts))\n",
    "\n",
    "        v.indices[meta_indices_to_corrupt] += offsets\n",
    "        return v, (meta_indices_to_corrupt, offsets)\n",
    "\n",
    "    def uncorrupt(self, visibles, state):\n",
    "        mitc, offsets = state\n",
    "        visibles.indices[mitc] -= offsets\n",
    "        \n",
    "class CharBernoulliSoftmax(CharBernoulli):\n",
    "    def __init__(self,**kwargs):\n",
    "        CharBernoulli.__init__(self, **kwargs)\n",
    "        \n",
    "    def _sample_visibles(self, h, temperature=1.0):\n",
    "        \"\"\"Sample from the distribution P(v|h). This obeys the softmax constraint\n",
    "        on visible units. i.e. sum(v) == softmax_shape[0] for any visible\n",
    "        configuration v.\n",
    "\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer to sample from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        \"\"\"\n",
    "        p = np.dot(h, self.components_/temperature)\n",
    "        p += self.intercept_visible_/(min(1.0, temperature) if BIASED_PRIOR else temperature)\n",
    "        nsamples, nfeats = p.shape\n",
    "        reshaped = np.reshape(p, (nsamples,) + self.softmax_shape)\n",
    "        return Utils.softmax_and_sample(reshaped).reshape((nsamples, nfeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "codec_kls = ShortTextCodec\n",
    "codec = codec_kls('',10,0,True,False)\n",
    "codec.debug_description()\n",
    "model_kwargs = {'codec': codec,\n",
    "                        'n_components': 100,\n",
    "                        'learning_rate': 0.1,\n",
    "                        'lr_backoff': False,\n",
    "                        'n_iter': 5,\n",
    "                        'verbose': 1,\n",
    "                        'batch_size': 10,\n",
    "                        'weight_cost': 0.0001,\n",
    "                        }\n",
    "kls = CharBernoulliSoftmax\n",
    "rbm = kls(**model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Names File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39800, 530) (17058, 530)\n"
     ]
    }
   ],
   "source": [
    "vecs = Utils.vectors_from_txtfile(\"./names.txt\", codec)\n",
    "train, validation = train_test_split(vecs, test_size=0.3)\n",
    "print(train.shape,validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing weights and biases\n",
      "[CharBernoulliSoftmax] Iteration 1/5\tt = 5.97s\n",
      "Pseudo-log-likelihood sum: -44249.33\tAverage per instance: -1.11\n",
      "E(vali):\t-11.48\tE(train):\t-11.52\tdifference: 0.04\n",
      "Fantasy samples: Corinski$$|Stote$$$$$|Sleilan$$$\n",
      "[CharBernoulliSoftmax] Iteration 2/5\tt = 6.51s\n",
      "Pseudo-log-likelihood sum: -41458.82\tAverage per instance: -1.04\n",
      "E(vali):\t-15.78\tE(train):\t-15.81\tdifference: 0.02\n",
      "Fantasy samples: Maromarse$|Wintlar$$$|Sillimana$\n",
      "[CharBernoulliSoftmax] Iteration 3/5\tt = 6.52s\n",
      "Pseudo-log-likelihood sum: -39801.02\tAverage per instance: -1.00\n",
      "E(vali):\t-18.65\tE(train):\t-18.70\tdifference: 0.05\n",
      "Fantasy samples: Dee$$$$$$$|Kielea$$$$|Basgert$$$\n",
      "[CharBernoulliSoftmax] Iteration 4/5\tt = 6.50s\n",
      "Pseudo-log-likelihood sum: -37956.15\tAverage per instance: -0.95\n",
      "E(vali):\t-21.23\tE(train):\t-21.31\tdifference: 0.08\n",
      "Fantasy samples: Beyeman$$$|Willer$$$$|Marnelli$$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CharBernoulliSoftmax()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.fit(train,validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pahes       \n",
      "Ezams       \n",
      "Tpeomen     \n",
      "Yot         \n",
      "Dittermand  \n",
      "Lyyen       \n",
      "Lish        \n",
      "Rotte       \n",
      "Renn        \n",
      "Tabey       \n",
      "Jeren       \n",
      "Bonkerman   \n",
      "Futterger   \n",
      "Rakat       \n",
      "Rei         \n",
      "Elher       \n",
      "Davet       \n",
      "Rena        \n",
      "Wuwen       \n",
      "Hayf        \n",
      "Chy         \n",
      "Plat        \n",
      "Luttn       \n",
      "Fine        \n",
      "Nella       \n",
      "Meany       \n",
      "Histeboale  \n",
      "Neal        \n",
      "Bloss       \n",
      "Mihel       \n",
      "Final energy: -29.02 (stdev=3.30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLES = []\n",
    "def horizontal_cb(strings, i, energy=None):\n",
    "    global SAMPLES\n",
    "    if energy is not None:\n",
    "        SAMPLES.append(zip(strings, energy))\n",
    "    else:\n",
    "        SAMPLES.append(strings)\n",
    "def print_columns(maxlen):\n",
    "    col_width = maxlen+2\n",
    "    for fantasy_index in range(len(SAMPLES[0])):\n",
    "        particles = [s[fantasy_index] for s in SAMPLES]\n",
    "        print (\"\".join(s[fantasy_index].ljust(col_width) for s in SAMPLES))\n",
    "sample_indices = [1000-1]\n",
    "kwargs = dict(start_temp=1.0, final_temp=1.0, sample_energy=False, \n",
    "                    callback=horizontal_cb)\n",
    "\n",
    "vis = Sampling.sample_model(rbm, 30, 1000, sample_indices, **kwargs)\n",
    "print_columns(rbm.codec.maxlen)\n",
    "fe = rbm._free_energy(vis)\n",
    "print('Final energy: {:.2f} (stdev={:.2f})\\n'.format(fe.mean(), fe.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish Name File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36523, 530) (15653, 530)\n"
     ]
    }
   ],
   "source": [
    "vecs = Utils.vectors_from_txtfile(\"./spanish.txt\", codec)\n",
    "train, validation = train_test_split(vecs, test_size=0.3)\n",
    "print(train.shape,validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing weights and biases\n",
      "[CharBernoulliSoftmax] Iteration 1/5\tt = 5.63s\n",
      "Pseudo-log-likelihood sum: -24017.08\tAverage per instance: -0.66\n",
      "E(vali):\t-38.12\tE(train):\t-38.18\tdifference: 0.07\n",
      "Fantasy samples: carucar$$$|atecuntar$|pubetear$$\n",
      "[CharBernoulliSoftmax] Iteration 2/5\tt = 6.01s\n",
      "Pseudo-log-likelihood sum: -21687.40\tAverage per instance: -0.59\n",
      "E(vali):\t-42.84\tE(train):\t-42.77\tdifference: -0.06\n",
      "Fantasy samples: empiluchir|preparizar|altertar$$\n",
      "[CharBernoulliSoftmax] Iteration 3/5\tt = 5.99s\n",
      "Pseudo-log-likelihood sum: -21348.57\tAverage per instance: -0.58\n",
      "E(vali):\t-43.01\tE(train):\t-43.14\tdifference: 0.12\n",
      "Fantasy samples: poldar$$$$|enriipar$$|rescitar$$\n",
      "[CharBernoulliSoftmax] Iteration 4/5\tt = 6.02s\n",
      "Pseudo-log-likelihood sum: -22206.58\tAverage per instance: -0.61\n",
      "E(vali):\t-48.46\tE(train):\t-48.52\tdifference: 0.06\n",
      "Fantasy samples: brinar$$$$|maquetear$|cerrintear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CharBernoulliSoftmax()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.fit(train,validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travacar    \n",
      "chamanar    \n",
      "ajeyar      \n",
      "emplefar    \n",
      "emprocar    \n",
      "trafanar    \n",
      "zarrars     \n",
      "embranar    \n",
      "trajar      \n",
      "adilar      \n",
      "mubtirar    \n",
      "sartabar    \n",
      "emblolar    \n",
      "embracar    \n",
      "pretener    \n",
      "promojir    \n",
      "tretar      \n",
      "subrecir    \n",
      "empracar    \n",
      "prorificar  \n",
      "clobar      \n",
      "silgonizar  \n",
      "arajar      \n",
      "merrerar    \n",
      "ahasar      \n",
      "pondar      \n",
      "zablar      \n",
      "tansuear    \n",
      "atarar      \n",
      "jirrar      \n",
      "Final energy: -64.08 (stdev=4.31)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLES = []\n",
    "def horizontal_cb(strings, i, energy=None):\n",
    "    global SAMPLES\n",
    "    if energy is not None:\n",
    "        SAMPLES.append(zip(strings, energy))\n",
    "    else:\n",
    "        SAMPLES.append(strings)\n",
    "def print_columns(maxlen):\n",
    "    col_width = maxlen+2\n",
    "    for fantasy_index in range(len(SAMPLES[0])):\n",
    "        particles = [s[fantasy_index] for s in SAMPLES]\n",
    "        print (\"\".join(s[fantasy_index].ljust(col_width) for s in SAMPLES))\n",
    "sample_indices = [1000-1]\n",
    "kwargs = dict(start_temp=1.0, final_temp=1.0, sample_energy=False, \n",
    "                    callback=horizontal_cb)\n",
    "\n",
    "vis = Sampling.sample_model(rbm, 30, 1000, sample_indices, **kwargs)\n",
    "print_columns(rbm.codec.maxlen)\n",
    "fe = rbm._free_energy(vis)\n",
    "print('Final energy: {:.2f} (stdev={:.2f})\\n'.format(fe.mean(), fe.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
