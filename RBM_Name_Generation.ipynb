{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Name Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from GridEncoder import GridEncoder\n",
    "import Utils\n",
    "from ShortTextCodec import ShortTextCodec, BinomialShortTextCodec\n",
    "from RBM import BernoulliRBM\n",
    "import Sampling\n",
    "import sample\n",
    "import sys\n",
    "import colorama\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent =  currentdir + '\\RBM_Git'\n",
    "sys.path.insert(0,parent)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numexpr  as ne\n",
    "import profile\n",
    "import rbm as Rbm\n",
    "import pandas\n",
    "from random import randint\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BIASED_PRIOR = 0\n",
    "\n",
    "class CharBernoulli(BernoulliRBM):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        codec is the ShortTextCodec used to create the vectors being fit. The\n",
    "        most important function of the codec is as a proxy to the shape of the\n",
    "        softmax units in the visible layer (if you're using the CharBernoulliRBMSoftmax\n",
    "        subclass). It's also used to decode and print\n",
    "        fantasy particles at the end of each epoch.\n",
    "        \"\"\"\n",
    "        # Attaching this to the object is really helpful later on when models\n",
    "        # are loaded from pickle in visualize.py and sample.py\n",
    "        self.codec = kwargs.pop(\"codec\")\n",
    "        self.softmax_shape = codec.shape()\n",
    "        # Old-style class :(\n",
    "        BernoulliRBM.__init__(self, **kwargs)\n",
    "\n",
    "    def wellness_check(self, epoch, duration, train, validation):\n",
    "        BernoulliRBM.wellness_check(self, epoch, duration, train, validation)\n",
    "        fantasy_samples = '|'.join([self.codec.decode(vec) for vec in\n",
    "                                    self._sample_visibles(self.h_samples_[:3], temperature=0.1)])\n",
    "        print (\"Fantasy samples: {}\".format(fantasy_samples))\n",
    "\n",
    "    def corrupt(self, v):\n",
    "        n_softmax, n_opts = self.softmax_shape\n",
    "        # Select a random index in to the indices of the non-zero values of each input\n",
    "        # TODO: In the char-RBM case, if I wanted to really challenge the model, I would avoid selecting any\n",
    "        # trailing spaces here. Cause any dumb model can figure out that it should assign high energy to\n",
    "        # any instance of /  [^ ]/\n",
    "        meta_indices_to_corrupt = self.rng_.randint(0, n_softmax, v.shape[0]) + np.arange(0, n_softmax * v.shape[0], n_softmax)\n",
    "\n",
    "        # Offset these indices by a random amount (but not 0 - we want to actually change them)\n",
    "        offsets = self.rng_.randint(1, n_opts, v.shape[0])\n",
    "        # Also, do some math to make sure we don't \"spill over\" into a different softmax.\n",
    "        # E.g. if n_opts=5, and we're corrupting index 3, we should choose offsets from {-3, -2, -1, +1}\n",
    "        # 1-d array that matches with meta_i_t_c but which contains the indices themselves\n",
    "        indices_to_corrupt = v.indices[meta_indices_to_corrupt]\n",
    "        # Sweet lucifer\n",
    "        offsets = offsets - (n_opts * (((indices_to_corrupt % n_opts) + offsets.ravel()) >= n_opts))\n",
    "\n",
    "        v.indices[meta_indices_to_corrupt] += offsets\n",
    "        return v, (meta_indices_to_corrupt, offsets)\n",
    "\n",
    "    def uncorrupt(self, visibles, state):\n",
    "        mitc, offsets = state\n",
    "        visibles.indices[mitc] -= offsets\n",
    "        \n",
    "class CharBernoulliSoftmax(CharBernoulli):\n",
    "    def __init__(self,**kwargs):\n",
    "        CharBernoulli.__init__(self, **kwargs)\n",
    "        \n",
    "    def _sample_visibles(self, h, temperature=1.0):\n",
    "        \"\"\"Sample from the distribution P(v|h). This obeys the softmax constraint\n",
    "        on visible units. i.e. sum(v) == softmax_shape[0] for any visible\n",
    "        configuration v.\n",
    "\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer to sample from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        \"\"\"\n",
    "        p = np.dot(h, self.components_/temperature)\n",
    "        p += self.intercept_visible_/(min(1.0, temperature) if BIASED_PRIOR else temperature)\n",
    "        nsamples, nfeats = p.shape\n",
    "        reshaped = np.reshape(p, (nsamples,) + self.softmax_shape)\n",
    "        return Utils.softmax_and_sample(reshaped).reshape((nsamples, nfeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codec_kls = ShortTextCodec\n",
    "codec = codec_kls('',10,0,True,False)\n",
    "codec.debug_description()\n",
    "model_kwargs = {'codec': codec,\n",
    "                        'n_components': 300,\n",
    "                        'learning_rate': 0.1,\n",
    "                        'lr_backoff': False,\n",
    "                        'n_iter': 500,\n",
    "                        'verbose': 1,\n",
    "                        'batch_size': 32,\n",
    "                        'weight_cost': 0.0001,\n",
    "                        }\n",
    "kls = CharBernoulliSoftmax\n",
    "rbm = kls(**model_kwargs)\n",
    "codec.alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Names File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = Utils.vectors_from_txtfile(\"./names.txt\", codec)\n",
    "train, validation = train_test_split(vecs, test_size=0.5)\n",
    "print(train.shape,validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rbm.fit(train,validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLES = []\n",
    "def horizontal_cb(strings, i, energy=None):\n",
    "    global SAMPLES\n",
    "    \n",
    "    if energy is not None:\n",
    "        SAMPLES.append(zip(strings, energy))\n",
    "    else:\n",
    "        SAMPLES.append(strings)\n",
    "        \n",
    "def print_columns(maxlen):\n",
    "    col_width = maxlen+2\n",
    "    for fantasy_index in range(len(SAMPLES[0])):\n",
    "        particles = [s[fantasy_index] for s in SAMPLES]\n",
    "        print (\"\".join(s[fantasy_index].ljust(col_width) for s in SAMPLES))\n",
    "        \n",
    "sample_indices = [1000-1]\n",
    "kwargs = dict(start_temp=1.0, final_temp=1.0, sample_energy=False, \n",
    "                    callback=horizontal_cb)\n",
    "\n",
    "vis = Sampling.sample_model(rbm, 30, 1000, sample_indices, **kwargs)\n",
    "print_columns(rbm.codec.maxlen)\n",
    "fe = rbm._free_energy(vis)\n",
    "print('Final energy: {:.2f} (stdev={:.2f})\\n'.format(fe.mean(), fe.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish Name File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codec_kls = ShortTextCodec\n",
    "codec = codec_kls('áéíóúñÁÉÍÓÚÑ',10,0,True,False)\n",
    "codec.debug_description()\n",
    "model_kwargs = {'codec': codec,\n",
    "                        'n_components': 300,\n",
    "                        'learning_rate': 0.1,\n",
    "                        'lr_backoff': False,\n",
    "                        'n_iter': 500,\n",
    "                        'verbose': 1,\n",
    "                        'batch_size': 32,\n",
    "                        'weight_cost': 0.0001,\n",
    "                        }\n",
    "print(codec.alphabet)\n",
    "kls = CharBernoulliSoftmax\n",
    "rbm = kls(**model_kwargs)\n",
    "vecs = Utils.vectors_from_txtfile(\"./spanish_Dict.txt\", codec)\n",
    "train, validation = train_test_split(vecs, test_size=0.5)\n",
    "print(train.shape,validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rbm.fit(train,validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLES = []\n",
    "def horizontal_cb(strings, i, energy=None):\n",
    "    global SAMPLES\n",
    "    if energy is not None:\n",
    "        SAMPLES.append(zip(strings, energy))\n",
    "    else:\n",
    "        SAMPLES.append(strings)\n",
    "def print_columns(maxlen):\n",
    "    col_width = maxlen+2\n",
    "    for fantasy_index in range(len(SAMPLES[0])):\n",
    "        particles = [s[fantasy_index] for s in SAMPLES]\n",
    "        print (\"\".join(s[fantasy_index].ljust(col_width) for s in SAMPLES))\n",
    "sample_indices = [1000-1]\n",
    "kwargs = dict(start_temp=1.0, final_temp=1.0, sample_energy=False, \n",
    "                    callback=horizontal_cb)\n",
    "\n",
    "vis = Sampling.sample_model(rbm, 30, 1000, sample_indices, **kwargs)\n",
    "print_columns(rbm.codec.maxlen)\n",
    "fe = rbm._free_energy(vis)\n",
    "print('Final energy: {:.2f} (stdev={:.2f})\\n'.format(fe.mean(), fe.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test with RBM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent =  currentdir + '\\RBM_Git'\n",
    "sys.path.insert(0,parent)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numexpr  as ne\n",
    "import profile\n",
    "import rbm as Rbm\n",
    "import pandas\n",
    "from random import randint\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method ShortTextCodec.debug_description of <ShortTextCodec.ShortTextCodec object at 0x000002532C52BC88>>\n"
     ]
    }
   ],
   "source": [
    "codec_kls = ShortTextCodec\n",
    "codec = codec_kls('áéíóúñÁÉÍÓÚÑ',10,0,True,False)\n",
    "print(codec.debug_description)\n",
    "vecs = Utils.vectors_from_txtfile(\"./spanish_Dict.txt\", codec)\n",
    "visible_dim = vecs.shape[1]\n",
    "hidden_dim = 200\n",
    "epochs = 100\n",
    "K = 1\n",
    "lr = 0.1\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((650, 200), (650,), <rbm.RBM at 0x2532c5ba160>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm_ = Rbm.RBM(visible_dim=visible_dim,\n",
    "               hidden_dim=hidden_dim,\n",
    "               seed=42,\n",
    "               mu=0, \n",
    "               sigma=0.3,\n",
    "               monitor_time=True)\n",
    "rbm_.W.shape, rbm_.b.shape, rbm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Data_Vector_Aux = np.array(vecs.toarray(), dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLast epoch:ime per epoch: 107.44\ttotal time: 107.45 0 \ttime per epoch: 107.44\ttotal time: 107.45\n",
      "\tTraining finished\n",
      "\n",
      "\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rbm_.fit(test_Data_Vector_Aux, \n",
    "         method='CDK',\n",
    "         K=K,\n",
    "         lr=0.01,\n",
    "         epochs=1,\n",
    "         batch_size=128,\n",
    "         plot_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLast epoch:ime per epoch: 4.05\ttotal time: 4.06 0 \ttime per epoch: 4.05\ttotal time: 4.06\n",
      "\tTraining finished\n",
      "\n",
      "\n",
      "Wall time: 4.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rbm_.fit(test_Data_Vector_Aux, \n",
    "         method='vectorized_CDK',\n",
    "         K=K,\n",
    "         lr=0.01,\n",
    "         epochs=1,\n",
    "         batch_size=128,\n",
    "         plot_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 0 \ttime per epoch: 4.04\ttotal time: 4.05 1 \ttime per epoch: 4.29\ttotal time: 8.35 2 \ttime per epoch: 4.27\ttotal time: 12.63 3 \ttime per epoch: 3.88\ttotal time: 16.52 4 \ttime per epoch: 3.69\ttotal time: 20.21 5 \ttime per epoch: 3.88\ttotal time: 24.09 6 \ttime per epoch: 3.80\ttotal time: 27.91 7 \ttime per epoch: 4.23\ttotal time: 32.14 8 \ttime per epoch: 3.64\ttotal time: 35.79 9 \ttime per epoch: 3.79\ttotal time: 39.58 10 \ttime per epoch: 4.44\ttotal time: 44.04 11 \ttime per epoch: 3.41\ttotal time: 47.45 12 \ttime per epoch: 3.41\ttotal time: 50.87 13 \ttime per epoch: 3.80\ttotal time: 54.68 14 \ttime per epoch: 3.56\ttotal time: 58.24 15 \ttime per epoch: 3.67\ttotal time: 61.91 16 \ttime per epoch: 4.04\ttotal time: 65.96 17 \ttime per epoch: 4.28\ttotal time: 70.25 18 \ttime per epoch: 4.43\ttotal time: 74.68 19 \ttime per epoch: 4.28\ttotal time: 78.97 20 \ttime per epoch: 4.48\ttotal time: 83.46 21 \ttime per epoch: 3.73\ttotal time: 87.20 22 \ttime per epoch: 3.87\ttotal time: 91.08 23 \ttime per epoch: 3.90\ttotal time: 94.98 24 \ttime per epoch: 3.44\ttotal time: 98.43 25 \ttime per epoch: 3.43\ttotal time: 101.86 26 \ttime per epoch: 3.42\ttotal time: 105.28 27 \ttime per epoch: 3.48\ttotal time: 108.77 28 \ttime per epoch: 3.46\ttotal time: 112.23 29 \ttime per epoch: 3.46\ttotal time: 115.70 30 \ttime per epoch: 3.43\ttotal time: 119.13 31 \ttime per epoch: 3.43\ttotal time: 122.56 32 \ttime per epoch: 3.44\ttotal time: 126.01 33 \ttime per epoch: 3.46\ttotal time: 129.48 34 \ttime per epoch: 3.46\ttotal time: 132.94 35 \ttime per epoch: 3.42\ttotal time: 136.36 36 \ttime per epoch: 3.45\ttotal time: 139.81 37 \ttime per epoch: 3.50\ttotal time: 143.31 38 \ttime per epoch: 4.30\ttotal time: 147.62 39 \ttime per epoch: 4.28\ttotal time: 151.91 40 \ttime per epoch: 4.34\ttotal time: 156.25 41 \ttime per epoch: 4.24\ttotal time: 160.50 42 \ttime per epoch: 4.16\ttotal time: 164.67 43 \ttime per epoch: 4.29\ttotal time: 168.96 44 \ttime per epoch: 4.23\ttotal time: 173.19 45 \ttime per epoch: 4.21\ttotal time: 177.41 46 \ttime per epoch: 4.20\ttotal time: 181.61 47 \ttime per epoch: 4.34\ttotal time: 185.96 48 \ttime per epoch: 4.33\ttotal time: 190.29 49 \ttime per epoch: 4.32\ttotal time: 194.61 50 \ttime per epoch: 4.32\ttotal time: 198.94 51 \ttime per epoch: 4.40\ttotal time: 203.34 52 \ttime per epoch: 4.25\ttotal time: 207.60 53 \ttime per epoch: 4.23\ttotal time: 211.83 54 \ttime per epoch: 4.25\ttotal time: 216.09 55 \ttime per epoch: 4.21\ttotal time: 220.31 56 \ttime per epoch: 4.12\ttotal time: 224.44 57 \ttime per epoch: 4.31\ttotal time: 228.75 58 \ttime per epoch: 4.40\ttotal time: 233.16 59 \ttime per epoch: 4.21\ttotal time: 237.38 60 \ttime per epoch: 4.31\ttotal time: 241.69 61 \ttime per epoch: 4.25\ttotal time: 245.94 62 \ttime per epoch: 4.26\ttotal time: 250.21 63 \ttime per epoch: 4.21\ttotal time: 254.42 64 \ttime per epoch: 4.13\ttotal time: 258.56 65 \ttime per epoch: 4.26\ttotal time: 262.83 66 \ttime per epoch: 4.16\ttotal time: 267.00 67 \ttime per epoch: 4.32\ttotal time: 271.32 68 \ttime per epoch: 4.13\ttotal time: 275.46 69 \ttime per epoch: 4.10\ttotal time: 279.57 70 \ttime per epoch: 4.27\ttotal time: 283.84 71 \ttime per epoch: 4.22\ttotal time: 288.06 72 \ttime per epoch: 4.20\ttotal time: 292.27 73 \ttime per epoch: 4.22\ttotal time: 296.49 74 \ttime per epoch: 4.07\ttotal time: 300.57 75 \ttime per epoch: 4.25\ttotal time: 304.83 76 \ttime per epoch: 4.21\ttotal time: 309.05 77 \ttime per epoch: 4.17\ttotal time: 313.22 78 \ttime per epoch: 4.19\ttotal time: 317.42 79 \ttime per epoch: 4.06\ttotal time: 321.48 80 \ttime per epoch: 4.13\ttotal time: 325.61 81 \ttime per epoch: 4.26\ttotal time: 329.87 82 \ttime per epoch: 4.16\ttotal time: 334.04 83 \ttime per epoch: 4.22\ttotal time: 338.27 84 \ttime per epoch: 4.23\ttotal time: 342.51 85 \ttime per epoch: 4.20\ttotal time: 346.72 86 \ttime per epoch: 4.07\ttotal time: 350.79 87 \ttime per epoch: 4.12\ttotal time: 354.91 88 \ttime per epoch: 4.16\ttotal time: 359.08 89 \ttime per epoch: 4.12\ttotal time: 363.21 90 \ttime per epoch: 4.16\ttotal time: 367.37 91 \ttime per epoch: 4.21\ttotal time: 371.59 92 \ttime per epoch: 4.21\ttotal time: 375.80 93 \ttime per epoch: 4.16\ttotal time: 379.96 94 \ttime per epoch: 4.28\ttotal time: 384.25 95 \ttime per epoch: 4.05\ttotal time: 388.31 96 \ttime per epoch: 4.11\ttotal time: 392.43 97 \ttime per epoch: 4.10\ttotal time: 396.54 98 \ttime per epoch: 4.11\ttotal time: 400.65 99 \ttime per epoch: 4.16\ttotal time: 404.82 100 \ttime per epoch: 4.15\ttotal time: 408.97 101 \ttime per epoch: 4.18\ttotal time: 413.15 102 \ttime per epoch: 4.11\ttotal time: 417.27 103 \ttime per epoch: 4.13\ttotal time: 421.41 104 \ttime per epoch: 4.25\ttotal time: 425.66 105 \ttime per epoch: 4.08\ttotal time: 429.75 106 \ttime per epoch: 4.23\ttotal time: 433.98 107 \ttime per epoch: 4.19\ttotal time: 438.17 108 \ttime per epoch: 4.16\ttotal time: 442.34 109 \ttime per epoch: 4.31\ttotal time: 446.65 110 \ttime per epoch: 4.28\ttotal time: 450.94 111 \ttime per epoch: 4.25\ttotal time: 455.19 112 \ttime per epoch: 4.23\ttotal time: 459.43 113 \ttime per epoch: 4.16\ttotal time: 463.59 114 \ttime per epoch: 4.32\ttotal time: 467.92 115 \ttime per epoch: 4.20\ttotal time: 472.12 116 \ttime per epoch: 4.15\ttotal time: 476.27 117 \ttime per epoch: 4.07\ttotal time: 480.35 118 \ttime per epoch: 4.21\ttotal time: 484.57 119 \ttime per epoch: 4.08\ttotal time: 488.65 120 \ttime per epoch: 4.15\ttotal time: 492.81 121 \ttime per epoch: 4.52\ttotal time: 497.34 122 \ttime per epoch: 4.73\ttotal time: 502.08 123 \ttime per epoch: 5.22\ttotal time: 507.30 124 \ttime per epoch: 5.27\ttotal time: 512.57 125 \ttime per epoch: 4.79\ttotal time: 517.37 126 \ttime per epoch: 4.42\ttotal time: 521.79 127 \ttime per epoch: 4.53\ttotal time: 526.32 128 \ttime per epoch: 4.18\ttotal time: 530.50 129 \ttime per epoch: 4.23\ttotal time: 534.73 130 \ttime per epoch: 4.17\ttotal time: 538.91 131 \ttime per epoch: 4.37\ttotal time: 543.28 132 \ttime per epoch: 4.79\ttotal time: 548.08 133 \ttime per epoch: 4.30\ttotal time: 552.39 134 \ttime per epoch: 4.28\ttotal time: 556.67 135 \ttime per epoch: 4.64\ttotal time: 561.32 136 \ttime per epoch: 4.99\ttotal time: 566.31 137 \ttime per epoch: 4.52\ttotal time: 570.84 138 \ttime per epoch: 4.32\ttotal time: 575.16 139 \ttime per epoch: 4.32\ttotal time: 579.48 140 \ttime per epoch: 4.38\ttotal time: 583.87 141 \ttime per epoch: 4.18\ttotal time: 588.05 142 \ttime per epoch: 4.11\ttotal time: 592.16 143 \ttime per epoch: 4.33\ttotal time: 596.50 144 \ttime per epoch: 4.29\ttotal time: 600.80 145 \ttime per epoch: 4.25\ttotal time: 605.05 146 \ttime per epoch: 4.23\ttotal time: 609.29 147 \ttime per epoch: 4.50\ttotal time: 613.80 148 \ttime per epoch: 4.25\ttotal time: 618.05 149 \ttime per epoch: 4.17\ttotal time: 622.22 150 \ttime per epoch: 4.13\ttotal time: 626.35 151 \ttime per epoch: 4.35\ttotal time: 630.71 152 \ttime per epoch: 4.42\ttotal time: 635.13 153 \ttime per epoch: 4.29\ttotal time: 639.43 154 \ttime per epoch: 4.63\ttotal time: 644.07 155 \ttime per epoch: 4.32\ttotal time: 648.40 156 \ttime per epoch: 4.15\ttotal time: 652.55 157 \ttime per epoch: 4.36\ttotal time: 656.92 158 \ttime per epoch: 4.15\ttotal time: 661.08 159 \ttime per epoch: 4.25\ttotal time: 665.33 160 \ttime per epoch: 4.41\ttotal time: 669.75 161 \ttime per epoch: 4.31\ttotal time: 674.06 162 \ttime per epoch: 4.76\ttotal time: 678.83 163 \ttime per epoch: 4.37\ttotal time: 683.21 164 \ttime per epoch: 4.13\ttotal time: 687.35 165 \ttime per epoch: 4.15\ttotal time: 691.51 166 \ttime per epoch: 4.48\ttotal time: 695.99 167 \ttime per epoch: 4.72\ttotal time: 700.72 168 \ttime per epoch: 4.18\ttotal time: 704.90 169 \ttime per epoch: 4.08\ttotal time: 708.99 170 \ttime per epoch: 4.34\ttotal time: 713.33 171 \ttime per epoch: 4.67\ttotal time: 718.00 172 \ttime per epoch: 4.67\ttotal time: 722.69 173 \ttime per epoch: 5.02\ttotal time: 727.72 174 \ttime per epoch: 4.75\ttotal time: 732.47 175 \ttime per epoch: 4.59\ttotal time: 737.07 176 \ttime per epoch: 4.64\ttotal time: 741.72 177 \ttime per epoch: 4.30\ttotal time: 746.02 178 \ttime per epoch: 4.77\ttotal time: 750.80 179 \ttime per epoch: 4.99\ttotal time: 755.79 180 \ttime per epoch: 4.49\ttotal time: 760.28 181 \ttime per epoch: 4.52\ttotal time: 764.81 182 \ttime per epoch: 4.29\ttotal time: 769.11 183 \ttime per epoch: 4.39\ttotal time: 773.50 184 \ttime per epoch: 4.56\ttotal time: 778.07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLast epoch:\ttime per epoch: 4.91\ttotal time: 782.99 186 \ttime per epoch: 5.39\ttotal time: 788.38 187 \ttime per epoch: 5.28\ttotal time: 793.67 188 \ttime per epoch: 4.56\ttotal time: 798.23 189 \ttime per epoch: 4.41\ttotal time: 802.64 190 \ttime per epoch: 4.09\ttotal time: 806.74 191 \ttime per epoch: 4.58\ttotal time: 811.33 192 \ttime per epoch: 4.67\ttotal time: 816.00 193 \ttime per epoch: 4.48\ttotal time: 820.48 194 \ttime per epoch: 5.15\ttotal time: 825.63 195 \ttime per epoch: 4.48\ttotal time: 830.12 196 \ttime per epoch: 4.39\ttotal time: 834.51 197 \ttime per epoch: 4.38\ttotal time: 838.90 198 \ttime per epoch: 4.21\ttotal time: 843.11 199 \ttime per epoch: 4.44\ttotal time: 847.56 199 \ttime per epoch: 4.44\ttotal time: 847.56\n",
      "\tTraining finished\n",
      "\n",
      "\n",
      "Wall time: 14min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rbm_.fit(test_Data_Vector_Aux, \n",
    "         method='vectorized_CDK',\n",
    "         K=K,\n",
    "         lr=0.01,\n",
    "         epochs=200,\n",
    "         batch_size=128,\n",
    "         plot_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650,)\n"
     ]
    }
   ],
   "source": [
    "word = codec.encode_onehot(\"acabar\")\n",
    "print(word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "ma??????$$\n"
     ]
    }
   ],
   "source": [
    "x_hat, x_hat_p = rbm_.sample_visible_from_visible(word, n_gibbs=2000)\n",
    "final_result = []\n",
    "for i in range(x_hat.shape[0]):\n",
    "    if x_hat[i] == True:\n",
    "        final_result.append(1)\n",
    "    else:\n",
    "        final_result.append(0)\n",
    "result = []\n",
    "result.append(final_result)\n",
    "result = np.asarray(result)\n",
    "result = result.ravel()\n",
    "print(result)\n",
    "print(codec.decode(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?os???$$$$\n"
     ]
    }
   ],
   "source": [
    "fW = codec.decode(x_hat)\n",
    "print(fW)\n",
    "# for i in range(10):\n",
    "#     x_hat1,x_hat_p1 = rbm_.sample_visible_from_visible(x_hat, n_gibbs=2000)\n",
    "#     print(codec.decode(x_hat1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN BERNOULLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "%matplotlib inline   \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm_Bernoulli = BernoulliRBM(n_components=100, learning_rate=0.01, random_state=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = Utils.vectors_from_txtfile(\"./spanish_Dict.txt\", codec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -40.73, time = 9.06s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -39.50, time = 9.74s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -39.88, time = 9.68s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -38.82, time = 9.71s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -37.48, time = 11.55s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -37.71, time = 10.01s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -37.60, time = 12.08s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -36.75, time = 11.66s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -36.95, time = 10.31s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -35.77, time = 10.59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.01, n_components=100, n_iter=10,\n",
       "       random_state=0, verbose=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm_Bernoulli.fit(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z?l?$$$$$$\n"
     ]
    }
   ],
   "source": [
    "test = rbm_Bernoulli.gibbs(vecs[0])\n",
    "test = test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformFromOneHotVector(vec):\n",
    "    finalWord = []\n",
    "    for k in range(len(vec)):\n",
    "        tmp = []\n",
    "        for i in range(len(vec[k])):\n",
    "            if vec[k][i] == 1:\n",
    "                index = i%maxLength\n",
    "                if(index != maxLength):\n",
    "                    tmp.append(index)\n",
    "        finalWord.append(tmp)\n",
    "    return finalWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mferrer\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25, 7, 19, 4, 13, 0, 13, 0, 13]]\n",
      "[[25, 13, 7, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 13, 17, 2, 25, 13, 13, 0, 13, 0, 13]]\n",
      "[[21, 25, 1, 7, 12, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 17, 4, 7, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 14, 18, 4, 13, 0, 13, 0, 13]]\n",
      "[[25, 13, 13, 14, 25, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 17, 1, 14, 13, 0, 13, 0, 13]]\n",
      "[[25, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 1, 7, 1, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 2, 1, 7, 25, 13, 0, 13, 0, 13]]\n",
      "[[25, 17, 19, 13, 0, 0, 13, 0, 13]]\n",
      "[[25, 7, 13, 0, 13, 13, 0, 13, 0, 13]]\n",
      "[[25, 14, 1, 7, 17, 17, 13, 0, 13, 0, 13]]\n",
      "[[20, 25, 2, 13, 0, 13, 0, 13]]\n",
      "[[6, 13, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 1, 13, 0, 13, 0, 13]]\n",
      "[[25, 13, 7, 11, 13, 18, 4, 13, 0, 13, 0, 13]]\n",
      "[[25, 13, 0, 13, 0, 13]]\n",
      "[[21, 25, 13, 7, 3, 17, 6, 21, 0, 10, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 2, 4, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 18, 12, 13, 14, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 1, 17, 13, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 17, 19, 0, 25, 13, 0, 13, 0, 13]]\n",
      "[[7, 12, 1, 2, 0, 1, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 2, 3, 17, 1, 17, 0, 13, 0, 13, 0, 13]]\n",
      "[[19, 25, 7, 0, 1, 0, 13, 0, 24, 13, 0, 13]]\n",
      "[[25, 7, 6, 17, 2, 18, 13, 0, 13, 0, 13]]\n",
      "[[1, 15, 17, 13, 13, 0, 13, 0, 13]]\n",
      "[[25, 4, 17, 25, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 1, 11, 13, 19, 13, 0, 13, 0, 13]]\n",
      "[[25, 13, 0, 15, 0, 1, 17, 13, 16, 0, 13, 0, 13]]\n",
      "[[24, 25, 7, 13, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 11, 13, 21, 0, 1, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 2, 4, 13, 0, 13, 16, 13, 0, 13]]\n",
      "[[25, 7, 13, 15, 18, 16, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 7, 15, 17, 18, 4, 13, 0, 13, 0, 13]]\n",
      "[[25, 17, 24, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 5, 7, 9, 12, 6, 1, 4, 18, 0, 13, 0, 13, 0, 13]]\n",
      "[[25, 1, 13, 2, 13, 0, 13, 0, 13]]\n",
      "(40, 650)\n"
     ]
    }
   ],
   "source": [
    "xx = vecs[:40].copy()\n",
    "maxLength = 26\n",
    "for ii in range(1000):\n",
    "    for n in range(40):\n",
    "        xx[n] = rbm_Bernoulli.gibbs(xx[n])\n",
    "for i in range(xx.shape[0]):\n",
    "    tmp = xx[i].toarray()\n",
    "    print(transformFromOneHotVector(tmp))\n",
    "print(xx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERNOULLI GRIDS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_fake(): \n",
    "    vec = [] \n",
    "    for i in range(1000): \n",
    "        tmp = [] \n",
    "        for k in range(48): \n",
    "            tmp.append(0) \n",
    "        vec.append(tmp) \n",
    "    return np.asarray(vec) \n",
    "\n",
    "grid_data = generate_data_fake()\n",
    "for i in range(1000):\n",
    "    grid_data[i][0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(255, sparse=True).fit(grid_data)\n",
    "oneHotData = oneHotEncoder.transform(grid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RBM_Machine = BernoulliRBM(n_components=250, learning_rate=0.01, n_iter=10, random_state=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -408.82, time = 9.01s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -85.96, time = 8.68s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -30.28, time = 8.16s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -18.74, time = 8.09s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -13.28, time = 9.24s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -9.87, time = 8.46s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -8.17, time = 7.87s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -7.04, time = 10.00s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -6.09, time = 9.27s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -5.42, time = 8.66s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.01, n_components=250, n_iter=10,\n",
       "       random_state=0, verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBM_Machine.fit(oneHotData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_visible = RBM_Machine.gibbs(oneHotData[0])\n",
    "x_visible = x_visible.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "final_vector = []\n",
    "for k in range(len(x_visible)):\n",
    "    if(x_visible[k] == True):\n",
    "        final_vector.append(1)\n",
    "    else:\n",
    "        final_vector.append(0)\n",
    "output_data = []\n",
    "for i in range(48):\n",
    "    tmp = final_vector[(255*i):((i+1)*255)]\n",
    "    output_data.append(tmp.index(1))\n",
    "print (output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
